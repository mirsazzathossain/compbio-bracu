{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/mirsazzathossain/compbio-bracu/blob/main/day_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## **Day 02: Interdisciplinary Computational Biology workshop 2025**\n",
    "\n",
    "### **Problem Statement:**\n",
    "\n",
    "In this hands-on workshop, you'll explore how machine learning can be applied to computational biology. You'll work with Gene Ontology (GO) terms that describe biological functions and processes. The task is to perform binary classification to predict the presence or absence of specific biological functions in a dataset.\n",
    "\n",
    "- You are provided with a list of GO terms in [this google sheet](https://docs.google.com/spreadsheets/d/1Sc_1Sfi4pKxQ6VGf4j7i7wp0u4kzREzyHb0qwj6AVbM/edit?usp=sharing). \n",
    "- Open the spreadsheet and select the GO terms that your group will work on. Before selecting, explore each term in the [QuickGO database](https://www.ebi.ac.uk/QuickGO/term/GO:0000001) to understand the biological context of the term.\n",
    "- You can use ChatGPT to help you understand the terms.\n",
    "\n",
    "\n",
    "In this notebook, you'll learn how to retrieve a dataset related to your selected GO terms and apply machine learning techniques to predict the presence or absence of these biological functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setup codebase**\n",
    "\n",
    "We will be using a Python package called `ProFAB` for to retrieve the GO dataset. Run the following code to get the package installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('profab'):\n",
    "    !git clone https://github.com/kansil/ProFAB.git\n",
    "    !cp -r ProFAB/profab .\n",
    "    !rm -rf ProFAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**\n",
    "\n",
    "To get started, we need to import a set of essential libraries that will help us perform various tasks:\n",
    "\n",
    "- **`numpy`**: For efficient numerical computations.\n",
    "- **`pandas`**: To handle and manipulate structured data.\n",
    "- **`matplotlib` and `seaborn`**: For creating insightful visualizations.\n",
    "- **`sklearn`**: To build and evaluate machine learning models.\n",
    "\n",
    "In Python, it's best practice to import all necessary libraries at the beginning of your script or notebook.  Do these later one by one when you need them in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Data**\n",
    "\n",
    "The `ProFAB` package provides the `GOID()` function, which allows you to retrieve datasets related to your selected Gene Ontology (GO) terms.\n",
    "\n",
    "**Parameters of the `GOID()` Function:**\n",
    "\n",
    "- **`ratio`**: Specifies the proportions for splitting the data into training, testing, and validation sets.\n",
    "  - If `ratio` is a single float (e.g., `0.2`), it defines the test set size, with the remaining data allocated to training.\n",
    "  - If `ratio` is a list of two floats (e.g., `[0.2, 0.1]`), it defines the test and validation set sizes, with the remaining data allocated to training.\n",
    "\n",
    "- **`protein_feature`**: Determines the numerical feature representation of proteins derived from their sequences. Options include:\n",
    "  - `'paac'`: Pseudo Amino Acid Composition (50 dimensions)\n",
    "  - `'aac'`: Amino Acid Composition (20 dimensions)\n",
    "  - `'gaac'`: Grouped Amino Acid Composition (5 dimensions)\n",
    "  - `'ctdt'`: CTD Translation (39 dimensions)\n",
    "  - `'socnumber'`: Quasi-Sequence-Order Coupling Number (60 dimensions)\n",
    "  - `'ctriad'`: Conjoint Triad (343 dimensions)\n",
    "  - `'kpssm'`: k-Separated-Bigrams POSSUM Vector (400 dimensions)\n",
    "\n",
    "  *Default: `'paac'`*\n",
    "\n",
    "- **`pre_determined`**: Indicates whether the data has been pre-split into training and testing sets.\n",
    "  - `False`: Data will be split according to the specified `ratio`.\n",
    "  - `True`: Pre-split data will be used.\n",
    "\n",
    "- **`set_type`**: Defines the method for splitting the data.\n",
    "  - `'random'`: Random splitting.\n",
    "  - `'similarity'`: Splitting based on protein similarity.\n",
    "  - `'temporal'`: Splitting based on the temporal aspect of data collection.\n",
    "\n",
    "    *Default: `'random'`*\n",
    "\n",
    "Use the `GOID()` function to initialize a data module and save it to a variable. \n",
    "\n",
    "\n",
    "**Retrieving Data for a Specific GO Term:**\n",
    "\n",
    "To retrieve data for a specific GO term, use the `get_data()` function. The function takes the following parameters:\n",
    "\n",
    "- **`data_name`**: The name of the GO term you selected. e.g., `'GO_0000001'`.\n",
    "\n",
    "\n",
    "**Purpose of Training and Testing Sets:**\n",
    "\n",
    "- **Training Set (`X_train`, `y_train`)**: Used to train the machine learning model, allowing it to learn the patterns and relationships within the data.\n",
    "- **Testing Set (`X_test`, `y_test`)**: Used to evaluate the model's performance on unseen data, providing an estimate of how well the model generalizes to new, unseen examples.\n",
    "\n",
    "Looking a bit complex? Don't worry! Just run the code below with your selected GO term, the data will be loaded and you can start exploring it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = GOID(ratio = 0.2, protein_feature = 'aac', pre_determined = False, set_type = 'random')\n",
    "X_train,X_test,y_train,y_test = data_model.get_data(data_name = 'GO_0000018')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explore Data**\n",
    "\n",
    "Now, we got the data loaded. Let's explore the data by looking at the shape of the data, the first few rows of the data. `FroFAB` package provides data as a list, we will convert the data into a pandas DataFrame for better visualization.\n",
    "\n",
    "- Preview the first few rows of the data using the `.head()` method.\n",
    "- Use `.info()` method to get the information about the data.\n",
    "- Use `.describe()` method to get the summary statistics of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(X_train)\n",
    "data['label'] = y_train\n",
    "\n",
    "# TODO: Display the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display the basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display some basic statistics of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Visualization**\n",
    "\n",
    "Visualizing data is a crucial step in the data analysis process. It helps you understand the data distribution, relationships between features, and more. Use the `pairplot()` function from the `seaborn` library to create a pairplot of the data. The pairplot shows the relationship between different features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wtire code to plot a pairplot of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Selection**\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant features for use in model construction. It helps to improve the model's performance by reducing overfitting and increasing the model's interpretability. Use correlation matrix to identify the features that are highly correlated with the target variable.\n",
    "\n",
    " **Correlation Matrix**:  \n",
    "   A correlation matrix helps identify the relationships between features. Correlation values range from -1 to 1:\n",
    "\n",
    "   - A value close to **1** indicates a strong positive correlation.\n",
    "   - A value close to **-1** indicates a strong negative correlation.\n",
    "   - A value close to **0** indicates weak or no correlation.\n",
    "\n",
    "   Use `df.corr()` to calculate the correlation matrix and `sns.heatmap()` to visualize it. Remove highly correlated features to reduce multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Calculate the correlation matrix of the dataset using the corr() method\n",
    "# Plot the correlation matrix using a heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Standardizing the Data**\n",
    "\n",
    "Standardization is important to scale the features so that they have a mean of 0 and a standard deviation of 1. This ensures that each feature contributes equally to the model, especially for algorithms sensitive to feature scales.\n",
    "\n",
    "**Action**:\n",
    "\n",
    "1. Use `StandardScaler` from `sklearn.preprocessing` to standardize the features.\n",
    "2. Apply the scaler to all features, excluding the target variable.\n",
    "3. Replace the original features with the standardized values in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Instantiate a StandardScaler object\n",
    "# Fit the scaler object on the training data\n",
    "# Transform the training and testing data using the scaler object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Building**\n",
    "\n",
    "Now that the data is preprocessed and ready, you can proceed with building a machine learning model.\n",
    "\n",
    "#### **Model Selection**\n",
    "\n",
    "For this task, you can choose any classification algorithm to build the model. Here are a few popular algorithms you can consider:\n",
    "\n",
    "**Traditional Classifiers:**\n",
    "\n",
    "- `LogisticRegression`\n",
    "- `KNeighborsClassifier`\n",
    "- `SVC` (Support Vector Classifier)\n",
    "- `DecisionTreeClassifier`\n",
    "- `RandomForestClassifier`\n",
    "- `GradientBoostingClassifier`\n",
    "- `AdaBoostClassifier`\n",
    "- `XGBClassifier` (if using `XGBoost`)\n",
    "\n",
    "**Deep Neural Network Classifier:**\n",
    "\n",
    "- `MLPClassifier` (Multi-layer Perceptron from `sklearn.neural_network`)\n",
    "\n",
    "Initialize the chosen model using appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate a Classifier object of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Training**\n",
    "\n",
    "Train the initialized model on the training data using the `fit()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit the classifier object on the training data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Predictions**\n",
    "\n",
    "Use the `predict()` function to predict the target values on the test set, and save them as `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict the labels of the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Evaluation**\n",
    "\n",
    "Evaluate the model's performance using various metrics such as accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "\n",
    "##### **Confusion Matrix**\n",
    "\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model on a set of data for which the true values are known. It helps visualize the performance of an algorithm.\n",
    "\n",
    "Use the `confusion_matrix()` function from `sklearn.metrics` to calculate the confusion matrix. Save the confusion matrix in a variable and visualize it using `sns.heatmap()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Calculate confusion matrix using of the predicted labels and the actual labels\n",
    "# Plot the confusion matrix using a heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Classification Metrics**\n",
    "\n",
    "There are several metrics to evaluate the performance of a classification model. Here are a few common metrics:\n",
    "\n",
    "- **Accuracy**: The proportion of correct predictions to the total number of predictions.\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "- **Precision**: The proportion of true positive predictions to the total positive predictions.\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "- **Recall**: The proportion of true positive predictions to the total actual positive instances.\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "- **F1-Score**: The harmonic mean of precision and recall.\n",
    "$$F1-Score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$\n",
    "\n",
    "Use the `classification_report()` function from `sklearn.metrics` to calculate these metrics and print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the classification_report method to calculate the precision, recall and f1-score of the model and print it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **ROC Curve**\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation of the true positive rate against the false positive rate. It shows the trade-off between sensitivity and specificity. The Area Under the Curve (AUC) is a metric that quantifies the overall performance of the model.\n",
    "\n",
    "Use the `roc_curve()` function from `sklearn.metrics` to calculate the ROC curve and the `auc()` function to calculate the AUC. Plot the ROC curve using `matplotlib`. \n",
    "For today's workshop, you just run the code below and get the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Logistic Regression ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Additional Tasks**\n",
    "\n",
    "- **Use Different Features**: While loading the data, we chosed `protein_feature` as `aac`. Now, for yur GO term, this feature may not be the best one. You task is to try with different features and see if the model performance improves. \n",
    "- **Choose Different Model**: You can try with different models and see if the model performance improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
